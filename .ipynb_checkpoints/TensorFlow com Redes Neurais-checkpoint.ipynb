{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R4ccyS7lNIUz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "R4T70anKSXn0",
        "outputId": "bc316c32-82f5-477a-8aa2-516d439a21f6"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Musical_instruments_reviews.csv\")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Verificando valores ausentes\n",
        "df.isnull().sum()\n",
        "\n",
        "print((df.isnull().sum() / len(df)) * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Análise dos Valores Ausentes\n",
        "    - reviewerName: 26.3% dos dados estão ausentes. Isso é uma quantidade significativa.\n",
        "    - reviewText: 6.8% dos dados estão ausentes. Como essa coluna contém o texto das avaliações, é aconselhável remover essas entradas, pois são essenciais para sua análise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Tratando valores ausentes\n",
        "df.dropna(subset=['reviewText'], inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df['reviewerName'].fillna('Unknown', inplace=True)\n",
        "\n",
        "print(df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Distribuição das Avaliações (overall)\n",
        "plt.figure(figsize=(8, 3))\n",
        "sns.countplot(data=df, x='overall')\n",
        "plt.title('Distribuição das Avaliações (overall)')\n",
        "plt.xlabel('Avaliação')\n",
        "plt.ylabel('Frequência')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.boxplot(data=df, y='overall')\n",
        "plt.title('Boxplot de Avaliações (overall)')\n",
        "plt.ylabel('Avaliação')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plotar a distribuição das avaliações ao longo do tempo\n",
        "df['reviewDate'] = pd.to_datetime(df['unixReviewTime'], unit='s')\n",
        "plt.figure(figsize=(12, 4))\n",
        "df.set_index('reviewDate').resample('M')['overall'].mean().plot()\n",
        "plt.title('Média das Avaliações ao Longo do Tempo')\n",
        "plt.xlabel('Data')\n",
        "plt.ylabel('Média da Avaliação')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"Text\"]  = df[\"summary\"] + \". \" + df[\"reviewText\"] \n",
        "\n",
        "df.drop([\"summary\", \"reviewText\", \"asin\", \"reviewerName\", \"reviewerID\", \"helpful\", \"unixReviewTime\", \"reviewTime\"], axis=1, inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Limpeza textual\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords = stopwords.words('english')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['cleaned_review'] = df['Text'].apply(clean_text)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_rating(rating):\n",
        "    if rating >= 4.0:\n",
        "        return 'positive'\n",
        "    elif rating == 3.0:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        return 'negative'\n",
        "\n",
        "df['sentiment'] = df['overall'].apply(classify_rating)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df[\"cleaned_review\"]\n",
        "y = df[\"sentiment\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test_vectorized = vectorizer.transform(X_test).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Lista de Modelos\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"SVM\": SVC()\n",
        "} \n",
        "results = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "for model_name, model in models.items():\n",
        "    model.fit(X_train_vectorized, y_train)\n",
        "    y_pred = model.predict(X_test_vectorized)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[model_name] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"classification_report\": classification_report(y_test, y_pred, output_dict=True)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for model_name, result in results.items():\n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"Accuracy: {result['accuracy']:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(result['classification_report'])\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análise do Modelo\n",
        "Melhor Acurácia:\n",
        "A Regressão Logística teve a melhor acurácia (0.8879) entre os modelos testados.\n",
        "Desempenho em Classes:\n",
        "    - Classe Positiva: A maioria dos modelos apresenta bom desempenho nessa classe, especialmente a Regressão Logística e o Gradient Boosting, com F1-scores acima de 0.94.\n",
        "    - Classe Neutra: Todos os modelos têm um desempenho fraco nesta classe, especialmente o SVM, que teve um F1-score de 0.0145.\n",
        "    - Classe Negativa: O desempenho é consistentemente baixo para todos os modelos, especialmente o SVM, que não conseguiu prever essa classe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Removendo classe neutral\n",
        "def classify_rating(rating):\n",
        "    if rating >= 4.0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "df['sentiment'] = df['overall'].apply(classify_rating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df[\"cleaned_review\"]\n",
        "y = df[\"sentiment\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {}\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train_vectorized, y_train)\n",
        "    y_pred = model.predict(X_test_vectorized)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[model_name] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"classification_report\": classification_report(y_test, y_pred, output_dict=True)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for model_name, result in results.items():\n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"Accuracy: {result['accuracy']:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(result['classification_report'])\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenização\n",
        "max_words = 5000\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding das sequências\n",
        "max_length = 100\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dividir os dados em características (X) e rótulos (y)\n",
        "X = df['cleaned_review']\n",
        "y = df['sentiment']\n",
        "\n",
        "# Dividir em conjuntos de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'Treinamento: {len(X_train)}, Teste: {len(X_test)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
        "\n",
        "# Definindo o modelo LSTM\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(max_words, 100, input_length=max_length))\n",
        "model_lstm.add(SpatialDropout1D(0.2))\n",
        "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model_lstm.add(Dense(1, activation='sigmoid'))  # Para 2 classes: positiva e negativa\n",
        "\n",
        "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Treinando o modelo\n",
        "model_lstm.fit(X_train_pad, y_train, epochs=5, batch_size=64, validation_data=(X_test_pad, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "\n",
        "# Definindo o modelo CNN\n",
        "model_cnn = Sequential()\n",
        "model_cnn.add(Embedding(max_words, 100, input_length=max_length))\n",
        "model_cnn.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "model_cnn.add(GlobalMaxPooling1D())\n",
        "model_cnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Treinando o modelo\n",
        "model_cnn.fit(X_train_pad, y_train, epochs=5, batch_size=64, validation_data=(X_test_pad, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tf-keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# Carregar o tokenizer BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenizar e padronizar os dados\n",
        "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=max_length)\n",
        "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "# Criar datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    y_train\n",
        ")).shuffle(1000).batch(16)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    y_test\n",
        ")).batch(16)\n",
        "\n",
        "# Definindo o modelo BERT\n",
        "model_bert = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Compilar o modelo\n",
        "model_bert.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), \n",
        "                   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "# Treinando o modelo\n",
        "model_bert.fit(train_dataset, epochs=3, validation_data=test_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Avaliar LSTM\n",
        "lstm_predictions = (model_lstm.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "print(classification_report(y_test, lstm_predictions))\n",
        "\n",
        "# Avaliar CNN\n",
        "cnn_predictions = (model_cnn.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "print(classification_report(y_test, cnn_predictions))\n",
        "\n",
        "# Avaliar BERT\n",
        "bert_predictions = model_bert.predict(test_dataset)\n",
        "bert_predictions = tf.argmax(bert_predictions.logits, axis=1).numpy()\n",
        "print(classification_report(y_test, bert_predictions))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
